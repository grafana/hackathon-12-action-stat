/////////////////////////////////////////////////////////////
// METRIC NAMING CONVENTIONS
/////////////////////////////////////////////////////////////
// Source: JSON (Authoritative)
// - workflow.duration: Total workflow execution time
//   Calculated as: (updatedAt - createdAt) for completed workflows
//   or (current_time - startedAt) for running workflows
// - workflow.job.duration: Job execution time
//   Calculated as: (completed_at - started_at) for completed jobs
//   or (current_time - started_at) for running jobs
// - workflow.step.duration: Step execution time
//   Calculated as: (completed_at - started_at) for completed steps
//   or (current_time - started_at) for running steps
// - workflow.status: Current workflow status
// - workflow.conclusion: Workflow conclusion
// - workflow.job.status: Job status
// - workflow.step.status: Step status

/////////////////////////////////////////////////////////////
// RECEIVERS
/////////////////////////////////////////////////////////////

// File log receiver for GitHub Actions workflow logs
otelcol.receiver.filelog "github_actions_logs" {
  include = [string.format("%s/*.log", env("LOGS_DIRECTORY"))]
  start_at = "beginning"
  delete_after_read = true
  
  resource = {
    "service.name" = "github_actions",
    "service.namespace" = env("GITHUB_REPOSITORY"),
    "workflow.name" = env("WORKFLOW_NAME"),
    "workflow.run_id" = env("WORKFLOW_RUN_ID"),
    "source" = "github_actions",
    "metric.source" = "logs",
  }

  operators = [
    // Parse job and step information from filename
    {
      type = "regex_parser",
      parse_from = "attributes[\"log.file.name\"]",
      regex = "^job-(?P<job_id>[0-9]+)(?:-step-(?P<step_number>[0-9]+))?\\.log$",
      on_error = "drop",
    },
    // Parse log line format - more lenient regex
    {
      type = "regex_parser",
      parse_from = "body",
      regex = "^(?:(?P<job_name>[^\\t]*?)\\t(?P<step_name>[^\\t]*?)\\t)?(?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]+Z).*$",
      on_error = "drop",
    },
    // Parse timestamp
    {
      type = "time_parser",
      parse_from = "attributes.timestamp",
      layout_type = "strptime",
      layout = "%Y-%m-%dT%H:%M:%S.%fZ",
      on_error = "drop",
    },
  ]

  output {
    logs = [otelcol.processor.batch.github_actions_batch.input]
  }
}

// File log receiver for workflow metrics
otelcol.receiver.filelog "github_actions_metrics" {
  include = [string.format("%s/*.log", env("METRICS_DIRECTORY"))]
  start_at = "beginning"
  delete_after_read = true
  
  resource = {
    "service.name" = "github_actions",
    "service.namespace" = env("GITHUB_REPOSITORY"),
    "workflow.name" = env("WORKFLOW_NAME"),
    "workflow.run_id" = env("WORKFLOW_RUN_ID"),
    "source" = "github_actions",
    "metric.source" = "json",
  }

  operators = [
    // First parse the entire body as JSON
    {
      type = "json_parser",
      parse_from = "body",
      parse_to = "attributes",
    },
    // Then parse timestamp from the now-available attributes
    {
      type = "time_parser",
      parse_from = "attributes.createdAt",
      layout_type = "layout",
      layout = "2006-01-02T15:04:05Z",
    },
    // Workflow level attributes
    {
      type = "move",
      from = "attributes.workflowName",
      to = "attributes.workflow.name",
    },
    {
      type = "move",
      from = "attributes.workflowDatabaseId",
      to = "attributes.workflow.id",
    },
    {
      type = "move",
      from = "attributes.databaseId",
      to = "attributes.run.id",
    },
    {
      type = "move",
      from = "attributes.status",
      to = "attributes.run.status",
    },
    {
      type = "move",
      from = "attributes.conclusion",
      to = "attributes.run.conclusion",
    },
    {
      type = "move",
      from = "attributes.duration",
      to = "attributes.workflow.duration",
    },
    // Job level attributes
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].name",
      to = "attributes.workflow.job.name",
    },
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].status",
      to = "attributes.workflow.job.status",
    },
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].duration",
      to = "attributes.workflow.job.duration",
    },
    // Step level attributes
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].steps[\"*\"].name",
      to = "attributes.workflow.step.name",
    },
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].steps[\"*\"].status",
      to = "attributes.workflow.step.status",
    },
    {
      type = "copy",
      from = "attributes.jobs[\"*\"].steps[\"*\"].duration",
      to = "attributes.workflow.step.duration",
    },
  ]

  output {
    metrics = [otelcol.exporter.otlphttp.destination.input]
  }
}

/////////////////////////////////////////////////////////////
// PROCESSORS
/////////////////////////////////////////////////////////////

// Batch processor for better performance
otelcol.processor.batch "github_actions_batch" {
  timeout = "1s"
  send_batch_size = 1024

  output {
    logs = [otelcol.processor.transform.github_actions_transform.input]
  }
}

// Transform processor to standardize attributes and convert to metrics
otelcol.processor.transform "github_actions_transform" {
  error_mode = "ignore"
  
  log_statements {
    context = "log"
    statements = [
      // Ensure consistent attribute naming for job fields (always present)
      "set(attributes[\"workflow.job.name\"], attributes[\"job_name\"]) where attributes[\"job_name\"] != nil",
      "set(attributes[\"workflow.job.id\"], attributes[\"job_id\"]) where attributes[\"job_id\"] != nil",
      // Only set step fields if they exist
      "set(attributes[\"workflow.step.name\"], attributes[\"step_name\"]) where attributes[\"step_name\"] != nil and attributes[\"step_name\"] != \"\"",
      "set(attributes[\"workflow.step.id\"], attributes[\"step_number\"]) where attributes[\"step_number\"] != nil and attributes[\"step_number\"] != \"\"",
      // Track timestamps for duration calculation - more comprehensive matching
      // Start timestamps - ordered by precedence
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"initialized workflow\")",
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"initialized job\")",
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"initialized step\")",
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"starting\")",
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"run job\")",
      "set(attributes[\"started_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"begin\")",
      // Completion timestamps - ordered by precedence
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"workflow complete\")",
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"job complete\")",
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"step complete\")",
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"completed\")",
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"done\")",
      "set(attributes[\"completed_at\"], attributes[\"timestamp\"]) where contains(lowercase(body), \"finish\")",
      // Add metric source
      "set(attributes[\"metric.source\"], \"logs\")",
      // Calculate durations for completed jobs/steps (ensure positive duration)
      "set(attributes[\"duration\"], max(0, time_diff_nano(attributes[\"started_at\"], attributes[\"completed_at\"]))) where attributes[\"started_at\"] != nil and attributes[\"completed_at\"] != nil",
      // Calculate durations for running jobs/steps (using current time)
      "set(attributes[\"duration\"], max(0, time_diff_nano(attributes[\"started_at\"], now()))) where attributes[\"started_at\"] != nil and attributes[\"completed_at\"] == nil",
      // Map status values (0=failed, 1=success, 2=in_progress, 3=cancelled, 4=skipped)
      // Terminal states (these take precedence)
      "set(attributes[\"status_value\"], 1) where contains(lowercase(body), \"completed successfully\")",
      "set(attributes[\"status_value\"], 0) where contains(lowercase(body), \"failed with exit code\")",
      "set(attributes[\"status_value\"], 0) where contains(lowercase(body), \"failure\")",
      "set(attributes[\"status_value\"], 3) where contains(lowercase(body), \"cancelled\")",
      "set(attributes[\"status_value\"], 3) where contains(lowercase(body), \"canceled\")",
      "set(attributes[\"status_value\"], 4) where contains(lowercase(body), \"skipped\")",
      // Non-terminal states (lower precedence)
      "set(attributes[\"status_value\"], 2) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"in progress\")",
      "set(attributes[\"status_value\"], 2) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"running\")",
      "set(attributes[\"status_value\"], 2) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"queued\")",
      "set(attributes[\"status_value\"], 2) where attributes[\"status_value\"] == nil and attributes[\"started_at\"] != nil and attributes[\"completed_at\"] == nil",
      // Generic states (lowest precedence)
      "set(attributes[\"status_value\"], 1) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"success\")",
      "set(attributes[\"status_value\"], 0) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"error\")",
      "set(attributes[\"status_value\"], 4) where attributes[\"status_value\"] == nil and contains(lowercase(body), \"disabled\")",
      // Ensure status values are set for workflow/job/step status attributes with proper scoping
      "set(attributes[\"workflow.status\"], coalesce(attributes[\"workflow.status\"], attributes[\"status_value\"])) where attributes[\"status_value\"] != nil and attributes[\"workflow.name\"] != nil",
      "set(attributes[\"workflow.job.status\"], coalesce(attributes[\"workflow.job.status\"], attributes[\"status_value\"])) where attributes[\"status_value\"] != nil and attributes[\"workflow.job.name\"] != nil",
      "set(attributes[\"workflow.step.status\"], coalesce(attributes[\"workflow.step.status\"], attributes[\"status_value\"])) where attributes[\"status_value\"] != nil and attributes[\"workflow.step.name\"] != nil",
      // Clean up temporary attributes
      "delete_key(attributes, \"job_name\")",
      "delete_key(attributes, \"step_name\") where attributes[\"step_name\"] != nil",
      "delete_key(attributes, \"job_id\")",
      "delete_key(attributes, \"step_number\") where attributes[\"step_number\"] != nil",
      "delete_key(attributes, \"timestamp\")",
      // Only clean up timestamps if we've calculated a duration
      "delete_key(attributes, \"started_at\") where attributes[\"duration\"] != nil",
      "delete_key(attributes, \"completed_at\") where attributes[\"duration\"] != nil",
      // Clean up status value after propagation
      "delete_key(attributes, \"status_value\") where attributes[\"workflow.status\"] != nil or attributes[\"workflow.job.status\"] != nil or attributes[\"workflow.step.status\"] != nil",
    ]
  }

  metric_statements {
    context = "datapoint"
    statements = [
      // Duration metrics (convert from nano to seconds)
      "set(name, \"workflow.duration\") where attributes[\"duration\"] != nil and attributes[\"workflow.name\"] != nil",
      "set(value_double, Divide(Int(attributes[\"duration\"]), 1000000000)) where name == \"workflow.duration\"",
      "set(metric_type, 2) where name == \"workflow.duration\"",  // GAUGE type
      "set(name, \"workflow.job.duration\") where attributes[\"duration\"] != nil and attributes[\"workflow.job.name\"] != nil",
      "set(value_double, Divide(Int(attributes[\"duration\"]), 1000000000)) where name == \"workflow.job.duration\"",
      "set(metric_type, 2) where name == \"workflow.job.duration\"",  // GAUGE type
      "set(name, \"workflow.step.duration\") where attributes[\"duration\"] != nil and attributes[\"workflow.step.name\"] != nil",
      "set(value_double, Divide(Int(attributes[\"duration\"]), 1000000000)) where name == \"workflow.step.duration\"",
      "set(metric_type, 2) where name == \"workflow.step.duration\"",  // GAUGE type
      // Status metrics with proper attribute checks
      "set(name, \"workflow.status\") where attributes[\"workflow.status\"] != nil",
      "set(value_int, Int(attributes[\"workflow.status\"])) where name == \"workflow.status\"",
      "set(metric_type, 2) where name == \"workflow.status\"",  // GAUGE type
      "set(name, \"workflow.conclusion\") where attributes[\"workflow.conclusion\"] != nil",
      "set(value_int, Int(attributes[\"workflow.conclusion\"])) where name == \"workflow.conclusion\"",
      "set(metric_type, 2) where name == \"workflow.conclusion\"",  // GAUGE type
      "set(name, \"workflow.job.status\") where attributes[\"workflow.job.status\"] != nil",
      "set(value_int, Int(attributes[\"workflow.job.status\"])) where name == \"workflow.job.status\"",
      "set(metric_type, 2) where name == \"workflow.job.status\"",  // GAUGE type
      "set(name, \"workflow.step.status\") where attributes[\"workflow.step.status\"] != nil",
      "set(value_int, Int(attributes[\"workflow.step.status\"])) where name == \"workflow.step.status\"",
      "set(metric_type, 2) where name == \"workflow.step.status\"",  // GAUGE type
      // Set units for duration metrics
      "set(unit, \"s\") where name matches \".*duration\"",
      // Set descriptions with proper status value documentation
      "set(description, \"Duration of workflow execution in seconds\") where name == \"workflow.duration\"",
      "set(description, \"Duration of job execution in seconds\") where name == \"workflow.job.duration\"",
      "set(description, \"Duration of step execution in seconds\") where name == \"workflow.step.duration\"",
      "set(description, \"Current workflow status (0=failed, 1=success, 2=in_progress, 3=cancelled, 4=skipped)\") where name == \"workflow.status\"",
      "set(description, \"Workflow conclusion (0=failed, 1=success, 3=cancelled, 4=skipped)\") where name == \"workflow.conclusion\"",
      "set(description, \"Current job status (0=failed, 1=success, 2=in_progress, 3=cancelled, 4=skipped)\") where name == \"workflow.job.status\"",
      "set(description, \"Current step status (0=failed, 1=success, 2=in_progress, 3=cancelled, 4=skipped)\") where name == \"workflow.step.status\"",
      // Propagate relevant attributes as dimensions
      "keep_keys(attributes, [\"workflow.name\", \"workflow.run_id\"]) where name == \"workflow.duration\" or name == \"workflow.status\" or name == \"workflow.conclusion\"",
      "keep_keys(attributes, [\"workflow.name\", \"workflow.run_id\", \"workflow.job.name\", \"workflow.job.id\"]) where name == \"workflow.job.duration\" or name == \"workflow.job.status\"",
      "keep_keys(attributes, [\"workflow.name\", \"workflow.run_id\", \"workflow.job.name\", \"workflow.job.id\", \"workflow.step.name\", \"workflow.step.id\"]) where name == \"workflow.step.duration\" or name == \"workflow.step.status\"",
    ]
  }

  output {
    metrics = [otelcol.exporter.otlphttp.destination.input]
  }
}

/////////////////////////////////////////////////////////////
// EXPORTERS
/////////////////////////////////////////////////////////////

// Common exporter configuration
otelcol.exporter.otlphttp "destination" {
  client {
    endpoint = env("TELEMETRY_URL")
    auth = otelcol.auth.basic.destination.handler
  }
}

otelcol.auth.basic "destination" {
  username = env("TELEMETRY_USERNAME")
  password = env("TELEMETRY_PASSWORD")
} 
